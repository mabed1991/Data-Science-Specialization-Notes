---
title: "6. Statistical Inference W2"
date: "5/8/2020"
output: 
  html_document:
    keep_md: true      
---

## Variance

- The variance of a random variable, as a measure of spread or dispersion, is, like a mean, defined as an expected value. It is the expected squared distance of the variable from its mean.  
Var(X) = E( (X-mu)^2 ) = E( (X-E(X))^2 ) = E(X^2)-E(X)^2

- the **sample variance** is an unbiased estimator of **population variance**.  

- if the population is infinite, the **variance of the sample mean is the population variance divided by the sample size**.  
Specifically, **Var(X') = sigma^2 / n**.  
where X' represents a sample mean, sigma represents population SD and n is the sample size.

- Var(X') =Var(1/n* Sum(X_i))=(1/n^2 )* Var(Sum(X_i))=(1/n^2 )*Sum(sigma^2 )=sigma^2 /n for infinite populations when our samples are independent.

- The standard deviation of a statistic is called its standard error, so the standard error of the sample mean is the square root of its variance.

## Common Distributions

### Bernoulli Distribution

- associated with experiments which have only 2 possible outcomes. These are also called (by people in the know) binary trials, such as Flipping a coin.

- Suppose we also specify that the probability that the Bernoulli outcome of 1 is p. The probability of a 0 outcome is then 1-p. and  PMF of a Bernoulli distribution is  
p^x * (1-p)^(1-x)

-  Suppose we have a Bernoulli random variable and, as before, the probability it equals 1 (a success) is p and probability it equals 0 (a failure) is 1-p.  
What is its mean? `p`  
what represents E(X^2)? `p`  
find variance:  
Var = E(X^2)-(E(X))^2 = p-p^2 =p*(1-p)  

### Binomial Distribution

- Binomial random variables are obtained as the sum of iid Bernoulli trials.  
Binomial random variables represent the number of successes, k, out of n independent Bernoulli trials. Each of the trials has probability p.  

- PMF of a binomial random variable X represent the probability that X=x. In other words, that there are x successes out of n independent trials.  
Here x, the number of successes, goes from 0 to n, the number of trials, and choose(n,x) represents the binomial coefficient 'n choose x' which is the number of ways x successes out of n trials can occur regardless of order.  
`choose(n,x) * p^x * (1-p)^(n-x)`

- Suppose we were going to flip a biased coin 5 times. The probability of tossing a head is .8 and a tail .2. What is the probability that you'll toss at least 3 heads.  

```{r Pbinom}
#using equation choose(n,x) * p^x * (1-p)^(n-x)
choose(5,3) * .8^3 * (1-.8)^(5-3)+choose(5,4) * .8^4 * (1-.8)^(5-4)+choose(5,5) * .8^5 * (1-.8)^(5-5)

# using R Pbinom function
pbinom(q = 2,size = 5,prob = .8,lower.tail = F)
```

### Normal or Gaussian distribution

- It has a complicated density function involving its mean mu and variance sigma^2. The key fact of the density formula is that when plotted, it forms a bell shaped curve, symmetric about its mean mu. The variance sigma^2 corresponds to the width of the bell, the higher the variance, the fatter the bell. We denote a normally distributed random variable X as X ~ N(mu, sigma^2).

- When mu = 0 and sigma = 1 the resulting distribution is called the standard normal distribution and it is often labeled Z.

- Approximately 68%, 95% and 99% of the normal density lie within 1, 2 and 3 standard deviations from the mean, respectively. These are shown in the three shaded areas of the figure. For example, the darkest portion (between -1 and 1) represents 68% of the area.



```{r normalD, echo=FALSE, fig.height=4, fig.width=5, warning=FALSE}
###  
x <- seq(-4, 4, length = 1000)
dat <- data.frame(x=x, y=dnorm(x))
library(ggplot2)
#to plot normal distribution:
#g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5)
#g <- g + geom_vline(xintercept = -4 : 4, size = 1)
#densities:
g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5)
g <- g + geom_ribbon(aes(x = ifelse(x > -1 & x < 1, x, 0), ymin = 0, ymax = dat$y), fill = "red", alpha = 1)
g <- g +  geom_ribbon(aes(x = ifelse(x > -2 & x < 2, x, 0), ymin = 0, ymax = dat$y), fill = "red", alpha = 0.5)
g <- g +  geom_ribbon(aes(x = ifelse(x > -3 & x < 3, x, 0), ymin = 0, ymax = dat$y), fill = "red", alpha = 0.35)
print(g)
```

- The R function qnorm(prob) returns the value of x (quantile) for which the area under the standard normal distribution to the left of x equals the parameter prob.  
Use qnorm now to find the 10th percentile of the standard normal.  
```{r qnorm}
qnorm(.1)
```

- We can use the symmetry of the bell curve to determine other quantiles. Given that 2.5% of the area under the curve falls to the left of x=-1.96, what is the 97.5 percentile for the standard normal?  
`1.96`

- Here are two useful facts concerning normal distributions. If X is a normal random variable with mean mu and variance sigma^2, i.e., X ~ N(mu,sigma^2),  
then the random variable Z defined as Z = (X -mu)/sigma is normally distributed with mean 0 and variance 1, i.e., Z ~ N(0, 1). (Z is standard normal.)

- The converse is also true. If Z is standard normal, i.e., Z ~ N(0,1), then the random variable X defined as X = mu + sigma*Z is normally distributed with mean mu and variance sigma^2, i.e., X ~ N(mu, sigma^2)

- We'll show how to find the 97.5th percentile of a normal distribution with mean 3 and variance 4.
```{r qnorm97}
qnorm(.975,3,2)

#or calc from standard normal then convert by sigma and mean
qnorm(.975)*2+3
```

- Suppose you have a normal distribution with mean 1020 and standard deviation of 50 and you want to compute the probability that the associated random variable X > 1200.

```{r pnorm}
pnorm(1200,1020,50,lower.tail = FALSE)
#or we can use the standard normal quantile instead
pnorm((1200-1020)/50,lower.tail = FALSE)
```

### Poisson Distribution

-  as Wikipedia tells us, "a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a known average rate and independently of the time since the last event."

- In other words, the Poisson distribution models counts or number of event in some interval of time. From Wikipedia, "Any variable that is Poisson distributed only takes on integer values."

- The PMF of the Poisson distribution has one parameter, lambda. As with the other distributions the PMF calculates the probability that the Poisson distributed random variable X takes the value x.  
Specifically, P(X=x)=(lambda^x)e^(-lambda)/x!. Here x ranges from 0 to infinity.

- The mean and variance of the Poisson distribution are both lambda.

- Poisson random variables are used to model rates such as the rate of hard drive failures. We write X~Poisson(lambda*t) where lambda is the expected count per unit of time and t is the total monitoring time.

- For example, suppose the number of people that show up at a bus stop is Poisson with a mean of 2.5 per hour, and we want to know the probability that at most 3 people show up in a 4 hour period. 

```{r ppoisExample}
ppois(3,2.5*4)
```

- The Poisson distribution approximates the binomial distribution in certain cases.  
Recall that the binomial distribution is the discrete distribution of the number of successes, k, out of n independent binary trials, each with probability p.  
If n is large and p is small then the Poisson distribution with lambda equal to n*p is a good approximation to the binomial distribution.

- Estimate the probability that you'll see at most 5 successes out of 1000 trials each of which has probability .01

```{r poisANDbinom}
#using binomial func:
pbinom(5,1000,.01)
#using pois func:
ppois(5,.01*1000)
```

