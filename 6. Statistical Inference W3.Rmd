---
title: "6. Statistical Inference W3"
date: "5/10/2020"
output: 
  html_document:
    keep_md: true      
---

## T Confidence Intervals

- In the Asymptotics lesson we mentioned the Z statistic Z=(X'-mu)/(sigma/sqrt(n)) which follows a standard normal distribution.

- Now we'll define the **t statistic** which looks a lot like the Z. It's defined as t=(X'-mu)/(s/sqrt(n)). Like the Z statistic, the t is centered around 0. The only difference between the two is that the population std deviation, sigma, in Z is *replaced by the sample standard deviation in the t*. So the distribution of the t statistic is independent of the population mean and variance. Instead it depends on the sample size n.

- As a result, for t distributions, the formula for computing a confidence interval is similar to what we did in the last lesson. However, instead of a **quantile** for a normal distribution we use a quantile for a **t distribution**. So the formula is Est +/- t-quantile *std error(Est). The other distinction, which we mentioned before, is that we'll use the sample standard deviation when we estimate the standard error of Est.

- These t confidence intervals are very **handy**, and if you have a choice between these and normal, pick these. We'll see that as datasets get larger, t-intervals look normal.

- The t distribution, invented by William Gosset in 1908, has **thicker tails** than the normal. Also, instead of having two parameters, mean and variance, as the normal does, the t distribution has **only one** - the number of degrees of freedom (df).

- As **df increases**, the t distribution gets more like a standard **normal**, so it's centered around 0. Also, the t *assumes that the underlying data are iid Gaussian so the statistic (X' - mu)/(s/sqrt(n)) has n-1 degrees of freedom*.

-  To see what we mean, the function **myplot**, which takes the integer df as its input and plots the t distribution with df degrees of freedom. It also plots a standard normal distribution so you can see how they relate to one another.

```{r plotDF,warning=FALSE}
library(ggplot2)
k <- 1000
xvals <- seq(-5, 5, length = k)
myplot <- function(df){
  d <- data.frame(y = c(dnorm(xvals), dt(xvals, df)),
                  x = xvals,
                  dist = factor(rep(c("Normal", "T"), c(k,k))))
  g <- ggplot(d, aes(x = x, y = y)) 
  g <- g + geom_line(size = 2, aes(colour = dist))
  print(g)
}
```

```{r plotDF2, fig.height=3, fig.width=4}
myplot(2)
```

- You can see that the hump of t distribution (in blue) is not as high as the normal's. Consequently, the two tails of the t distribution absorb the extra mass, so they're **thicker** than the normal's. Note that with 2 degrees of freedom, you only have 3 data points. Ha! Talk about small sample sizes. Now try myplot with an input of 20.

```{r plotDF20, fig.height=3, fig.width=4}
myplot(20)
```

- The two distributions are almost right on top of each other using this higher degree of freedom.

- Another way to look at these distributions is to **plot their quantiles**. We've provided a second function, myplot2, which does this. It plots a lightblue reference line representing normal quantiles and a black line for the t quantiles. Both plot the quantiles starting at the 50th percentile which is 0 (since the distributions are symmetric about 0) and go to the 99th.

```{r tqQnorm}
pvals <- seq(.5, .99, by = .01)
myplot2 <- function(df){
  d <- data.frame(n= qnorm(pvals),t=qt(pvals, df),
                  p = pvals)
  g <- ggplot(d, aes(x= n, y = t))
  g <- g + geom_abline(size = 2, col = "lightblue")
  g <- g + geom_line(size = 2, col = "black")
  g <- g + geom_vline(xintercept = qnorm(0.975))
  g <- g + geom_hline(yintercept = qt(0.975, df))
  print(g)
}
```


```{r qt2, fig.height=3, fig.width=4}
myplot2(2)
```

- The distance between the two thick lines represents the difference in sizes between the quantiles and hence the two sets of intervals.

- Note the thin horizontal and vertical lines. These represent the .975 quantiles for the t and normal distributions respectively.  
Check the placement of the horizontal:

```{r qt975}
qt(.975,2)
```

- Now run myplot2 with an argument of 20.

```{r qt20, fig.height=3, fig.width=4}
myplot2(20)
```

- The quantiles are much **closer** together with the **higher degrees of freedom**. At the 97.5 percentile, though, the t quantile is still greater than the normal. Student's Rules!

- This means the the t interval is always **wider** than the normal. This is because estimating the standard deviation introduces more uncertainty so a wider interval results.

- So the **t-interval** is defined as X' +/- t_(n-1)*s/sqrt(n) where t_(n-1) is the relevant quantile.  
The t interval assumes that the data are iid normal, though it is robust to this assumption and works well whenever the distribution of the data is roughly symmetric and mound shaped.

### paired observations

- Although it's pretty great, the t interval isn't always applicable. For **skewed** distributions, the spirit of the t interval assumptions (being centered around 0) are violated. There are ways of working around this problem (such as taking logs or using a different summary like the median).  
For highly discrete data, like binary, intervals other than the t are available.



- However, paired observations are often analyzed using the t interval by taking differences between the observations. We'll show you what we mean now.

- we're going to look at some sleep data. This was the data originally analyzed in Gosset's Biometrika paper, which shows the increase in hours for 10 patients on two soporific drugs. 

- R treats it as two groups rather than paired. To see what we mean type sleep now. 

```{r sleep}
data(sleep)
sleep
```

- We see 20 entries, the first 10 show the results (extra) of the first drug (group 1) on each of the patients (ID), and the last 10 entries the results of the second drug (group 2) on each patient (ID). 

```{r sleepplot, fig.height=5, fig.width=4}
g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))
g <- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = "salmon", alpha = .5)
print(g)
```


- To clarify:
```{r sleep1}
g1 <- sleep$extra[1 : 10]
g2 <- sleep$extra[11 : 20]

range(g1)
range(g2)
difference <- g2-g1
mean(difference)

```

- See how much smaller the mean difference in this paired data is compared to the group variations!


```{r sleep2}
# standard deviation of  difference
s <- sd(difference)
```

- Now recall the formula for finding the t confidence interval, X' +/- t_(n-1)* s/sqrt(n). Make the appropriate substitutions to **find the 95% confidence intervals** for the average difference you just computed.
  
``` {r sleep3} 
#Make the appropriate substitutions to find the 95% confidence intervals for the average difference you just computed.
n1 <- length(g1)
n2 <- length(g2)
md <- mean(g2)-mean(g1)
mn <- mean(g2-g1)

mn + c(-1,1)*qt(.975,9)*s/sqrt(10)
```
  
- This says that with probability .95 the average difference of effects (between the two drugs) for an individual patient is **between .7 and 2.46** additional hours of sleep.

- We could also just have used the R function t.test with the argument difference to get this result.
``` {r sleep4}
t.test(difference)$conf.int
```




### Independent Groups
  
- Suppose that we want to compare the mean **blood pressure** between two groups in a randomized trial. We'll compare those who received the treatment to those who received a placebo. Unlike the sleep study, we cannot use the paired t test because the groups are independent and may have different sample sizes.

- So our goal is to find a **95% confidence interval** of the difference between two population means. Let's represent this difference as mu_y - mu_x. How do we do this? Recall our formula X' +/- t_(n-1)*s/sqrt(n).

- First we need a **sample mean**, but we have two, X' and Y', one from each group. It makes sense that we'd have to take their difference (Y'-X') as well, since we're looking for a confidence interval that contains the difference mu_y-mu_x.  

- Now we need to specify a t **quantile**. Suppose the groups have different sizes n_x and n_y. For one group we used the  quantile t_(.975,n-1). What do you think we'll use for the quantile of this problem?  
    `t_(.975,n_x+n_y-2)`
  

- The only term remaining is the **standard error** which for the single group is s/sqrt(n). Let's deal with the numerator first. Our interval will assume (for now) a common variance s^2 across the two groups. We'll actually **pool variance** information from the two groups using a weighted sum. (We'll deal with the more complicated situation later.) 

- We call the variance estimator we use the pooled variance. The formula for it requires two variance estimators (in the form of the standard deviation), S_x and S_y, one for each group. *We multiply each by its respective degrees of freedom and divide the sum by the total number of degrees of freedom*. This  weights the respective variances; those coming from bigger samples get more weight.

- numerator: `(n_x-1)(S_x)^2+(n_y-1)(S_y)^2`

- the total number of degrees of freedom `(n_x-1)+(n_y-1)`
 
$$
    \begin{eqnarray*}
    E[S_p^2] & = & \frac{(n_x - 1) E[S_x^2] + (n_y - 1) E[S_y^2]}{n_x + n_y - 2}\\
            & = & \frac{(n_x - 1)\sigma^2 + (n_y - 1)\sigma^2}{n_x + n_y - 2}
    \end{eqnarray*}
$$

- Now recall we're calculating the **standard error** term which for the single group case was s/sqrt(n). We've got the numerator done, by pooling the sample variances. How do we handle the 1/sqrt(n) portion? We can simply *add 1/n_x and 1/n_y and take the square root of the sum. Then we MULTIPLY this by the sample variance* to complete the estimate of the standard error.

$$
\sigma^2 (\frac{1}{n_x} + \frac{1}{n_y})
$$


- Now we'll plug in some numbers based on an *example* from Rosner's book Fundamentals of Biostatistics, a very good, if heavy, reference book. We want to compare blood pressure from two independent groups.  

- The first is a group of 8 oral contraceptive users and the second is a group of 21 controls. The two means are` X'_{oc}=132.86` and `X'_{c}=127.44`, and the two sample standard deviations are `s_{oc}= 15.34` and `s_{c}= 18.23`. 

```{r bloodPressureEx}
# Let's first compute the numerator of the **pooled sample variance** by weighting the sum of the two by their respective sample sizes. Recall the formula (n_x-1)(S_x)^2+(n_y-1)(S_y)^2 

sp <- 7*15.34^2 + 20*18.23^2

# Now how many **degrees of freedom** are there? Put your answer in the variable ns.

ns <- 8+21-2
  
# Now divide sp by ns, take the square root and put the result back in sp.
  
sp <- sqrt(sp/ns)

# Now to find the 95% confidence interval. 

132.86-127.44+c(-1,1)*qt(.975,ns)*sp*sqrt(1/8+1/21)
```


- Notice that **0 is contained** in this 95% interval. That means that you can't rule out that the means of the two groups are equal since a difference of 0 is in the interval.



- Let's **revisit the sleep problem** and instead of looking at the data as paired over 10 subjects we'll look at it as two **independent** sets each of size 10. Recall the data is stored in the two vectors g1 and g2; we've also stored the difference between their means in the variable md. 

```{r sleepIndependent}
# Let's compute the sample pooled variance and store it in the variable sp. 
sp <- sqrt((9*var(g1)+9*var(g2))/18)

# Now  the last term of the formula, the standard error of the mean difference, is simply sp times the square root of the sum 1/10 + 1/10

# Find the 95% t confidence interval
 md + c(-1,1)*qt(.975,18)*sp*sqrt(1/10 + 1/10)
```

- We can check this manual calculation against the R function **t.test**.

```{r sleepttest}
#Since we subtracted g1 from g2, be sure to place g2 as your first argument and g1 as your second. Also make sure the argument paired is FALSE and var.equal is TRUE. We only need the confidence interval so use the construct x$conf.  Do this now.
 t.test(g2,g1,paired=FALSE,var.equal=TRUE)$conf


```
 
- Pretty cool that it matches, right? Note that **0** is again in this 95% interval so you **can't reject** the claim that the two groups are the same. (Recall that this is the **opposite** of what we saw with **paired data**.)

- Let's run t.test again, this time with **paired=TRUE** and see how different the result is. 

```{r sleepttest2}
#Don't specify var.equal and look only at the confidence interval.
t.test(g2,g1,paired=TRUE)$conf

```

- Just as we saw when we ran t.test on our vector, difference! See how the interval **excludes 0**? This means the groups *when paired have much different averages*.  

### Two Groups with Unequal Variances

- Now let's talk about calculating confidence intervals for two groups which have unequal variances. We won't be pooling them as we did before.

- In this case the formula for the interval is similar to what we saw before, `Y'-X' +/- t_df * SE` , where as before Y'-X' represents the difference of the sample means. However, the standard error SE and the quantile t_df are calculated differently from previous methods.  
Here SE is the square root of the sum of the squared standard errors of the two means, `(s_1)^2/n_1 + (s_2)^2/n_2` .
Under unequal variances
$$
    \bar Y - \bar X \sim N\left(\mu_y - \mu_x, \frac{s_x^2}{n_x} + \frac{\sigma_y^2}{n_y}\right)
$$
- When the underlying X and Y data are iid normal and the variances are different, the normalized **statistic** we started this lesson with, `(X'-mu)/(s/sqrt(n))`, **doesn't follow a t distribution**. However, it can be **approximated** by a t distribution if we set the degrees of freedom appropriately. 


- The statistic 
$$
    \frac{\bar Y - \bar X - (\mu_y - \mu_x)}{\left(\frac{s_x^2}{n_x} + \frac{\sigma_y^2}{n_y}\right)^{1/2}}
$$
approximately follows Gosset's $t$ distribution with degrees of freedom equal to
$$
    \frac{\left(S_x^2 / n_x + S_y^2/n_y\right)^2}
    {\left(\frac{S_x^2}{n_x}\right)^2 / (n_x - 1) +
      \left(\frac{S_y^2}{n_y}\right)^2 / (n_y - 1)}
$$


- Let's plug in the numbers from the **blood pressure study** to see how this works. Recall we have two groups, the first with size `8` and` X'_{oc}=132.86 `and `s_{oc}=15.34` and the second with size `21` and` X'_{c}=127.44` and `s_{c}=18.23`. 

```{r bloodUnequalVariance}
#Let's compute the degrees of freedom first. Start with the numerator
num <- (15.34^2/8 + 18.23^2/21)^2

#Now the denominator. This is the sum of two terms. Each term has the form s^4/n^2/(n-1).
den <- 15.34^4/8^2/7 + 18.23^4/21^2/20

# Now divide num by den and put the result in mydf.
mydf <- num/den

#Now with the R function qt(.975,mydf) compute the 95% t interval.
#Recall the formula. X'_{oc}-X'_{c} +/- t_df * SE. 
#Recall that SE is the square root of the sum of the squared standard errors of the two means, (s_1)^2/n_1 + (s_2)^2/n_2 .
132.86-127.44 +c(-1,1)*qt(.975,mydf)*sqrt(15.34^2/8 + 18.23^2/21)
```


- R makes things a lot **easier!**. If you call t.test with **var.equal** set to FALSE, then R calculates the  degrees of freedom for you. You don't have to memorize the formula.

---

## Hypothesis Testing

* Hypothesis testing is concerned with making decisions using data
* A null hypothesis is specified that represents the status quo, usually labeled $H_0$
* The null hypothesis is assumed true and statistical evidence is required to reject it in favor of a research or alternative hypothesis 


### Sleep Example

* A respiratory disturbance index of more than $30$ events / hour, say, is considered evidence of severe sleep disordered breathing (SDB).
* Suppose that in a sample of $100$ overweight subjects with other risk factors for sleep disordered breathing at a sleep clinic, the mean RDI was $32$ events / hour with a standard deviation of $10$ events / hour.
* We might want to test the hypothesis that 
  * $H_0 : \mu = 30$
  * $H_a : \mu > 30$
  * where $\mu$ is the population mean RDI.

* The **alternative** hypotheses are typically of the form $<$, $>$ or $\neq$
* Note that there are four possible outcomes of our statistical decision process

Truth | Decide | Result |
---|---|---|
$H_0$ | $H_0$ | Correctly accept null |
$H_0$ | $H_a$ | Type I error |
$H_a$ | $H_a$ | Correctly reject null |
$H_a$ | $H_0$ | Type II error |


- We distinguish between these **two errors**. A Type I error REJECTS a TRUE null hypothesis H_0 and a Type II error ACCEPTS a FALSE null hypothesis H_0. 


### Error Example

* Consider a **court of law**; the null hypothesis is that the defendant is innocent
* We require evidence to reject the null hypothesis (convict)
* If we require little evidence, then we would increase the percentage of innocent people convicted (type I errors); however we would also increase the percentage of guilty people convicted (correctly rejecting the null)
* If we require a lot of evidence, then we increase the the
  percentage of innocent people let free (correctly accepting the
  null) while we would also increase the percentage of guilty people
  let free (type II errors)


- Since there's  some element of uncertainty in questions concerning populations, we deal with probabilities. In our hypothesis testing we'll set the probability of making errors small. For now we'll **focus on Type I errors**, rejecting a correct hypothesis.

- The probabilities of making these **two kinds of errors are related**. If you decrease the probability of making a Type I error (rejecting a true hypothesis), you increase the probability of making a Type II error (accepting a false one) and vice versa. 

### Sleep Example continued

- A reasonable strategy would reject the null hypothesis if our sample mean X' was larger than some constant C. We choose C so that the probability of a **Type I error, alpha, is .05** (or some other favorite constant). Many scientific papers use .05 as a standard level of rejection.

- This means that  **alpha, the Type I error** rate, is the probability of rejecting the null hypothesis when, in fact, it is correct. We don't want alpha too low because then we would never reject the null hypothesis even if it's false.

-  Recall that the **standard error** of a sample mean is given by the formula s/sqrt(n). Recall in our sleep example we had a sample of 100 subjects, our mean RDI (X') was 32 events / hour with a standard deviation (s) of 10 events / hour. What is the standard error of the mean in this example?  
  `1`
  

- Under H_0, X' is normally distributed with mean mu=30 and variance 1. (We're estimating the variance as the square of the standard error which in this case is 1.) We want to **choose the constant C** so that the probability that X is greater than C given H_0 is 5%. That is, P(X > C| H_0) is 5%. Sound familiar? 

  
$$
\begin{align}
0.05  & =  P\left(\bar X \geq C ~|~ \mu = 30 \right) \\
      & =  P\left(\frac{\bar X - 30}{10 / \sqrt{100}} \geq \frac{C - 30}{10/\sqrt{100}} ~|~ \mu = 30\right) \\
      & =  P\left(Z \geq \frac{C - 30}{1}\right) \\
\end{align}
$$




- Here's a plot to show what we mean. The shaded portion represents 5% of the area under the curve and those X values in it are those for which the probability that X>C is 5%. 

  
```{r plot.05, echo=FALSE, fig.height=4, fig.width=4}
x <- seq(-8,8, length = 2000)
dat <- data.frame(x=x, y=dnorm(x))
g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5) + xlim(-4, 4) + scale_y_continuous(limits=c(0,max(dat$y)))
suppressWarnings(g <- g+ layer("area", stat="identity", position="identity",mapping = aes(x=ifelse(x>qnorm(.95),x,NA)),
            params=list(fill="red",alpha=.5, na.rm=TRUE)) )
suppressWarnings(print(g))
```



- The **95th percentile** of a standard normal distribution is 1.645 standard deviations from the mean, so in our example  we have to set C to be 1.645 standard deviations MORE than our hypothesized mean of 30, that is,  C = 30 +  1.645 * 1 = 31.645 (recall that the variance and standard deviation equalled 1).

- This means that if our OBSERVED (sample) mean X' >= C, then it's only a **5% chance** that a random draw from this N(30,1) distribution is larger than C.


* Hence $(C - 30) / 1 = 1.645$ implying $C = 31.645$
* Since our mean is $32$ we **reject** the null hypothesis


### Sleep Example (Z score)
* In general we don't convert $C$ back to the original scale
* We would just reject because the Z-score; which is how many standard errors the sample mean is above the hypothesized mean
  
  $$
  \frac{32 - 30}{10 / \sqrt{100}} = 2
  $$
  
  is greater than $1.645$
* Or, whenever $\sqrt{n} (\bar X - \mu_0) / s > Z_{1-\alpha}$

### General rules
* The $Z$ test for $H_0:\mu = \mu_0$ versus 
  * $H_1: \mu < \mu_0$
  * $H_2: \mu \neq \mu_0$
  * $H_3: \mu > \mu_0$ 
* Test statistic $TS = \frac{\bar{X} - \mu_0}{S / \sqrt{n}}$
* Reject the null hypothesis when 
  * $TS \leq -Z_{1 - \alpha}$
  * $|TS| \geq Z_{1 - \alpha / 2}$
  * $TS \geq Z_{1 - \alpha}$





- Here's a plot to show what we mean. The shaded portion represents 5% of the area under the curve and those X values in it are those which are at least 1.64 standard deviations **less than**  the mean. The probability of this is 5%. This means that if our sample mean fell in this area, we would reject a true null hypothesis,  mu=mu_0, with probability 5%.
 
```{r left.05, echo=FALSE, fig.height=4, fig.width=5}
x <- seq(-8,8, length = 1000)
dat <- data.frame(x=x, y=dnorm(x))
g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5) + xlim(-4, 4) + scale_y_continuous(limits=c(0.0,max(dat$y)))
suppressWarnings(g <- g+ layer("area", stat="identity", position="identity",mapping = aes(x=ifelse(-9<x & x<qnorm(.05),x,NA)),
            params=list(fill="red",alpha=.5, na.rm=TRUE)) )
suppressWarnings(print(g))
```
 
 

- Here again is the plot to show this. The shaded portion represents 5% of the area under the curve and those X values in it are those which are at least 1.64 standard deviations **greater than** the mean. The probability of this is 5%. This means that if our observed mean fell in this area we would reject a true null hypothesis, that mu=mu_0, with probability  5%.
  
```{r right.05, echo=FALSE, fig.height=4, fig.width=5}
x <- seq(-8,8, length = 2000)
dat <- data.frame(x=x, y=dnorm(x))
g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5) + xlim(-4, 4) + scale_y_continuous(limits=c(0,max(dat$y)))
suppressWarnings(g <- g+ layer("area", stat="identity", position="identity",mapping = aes(x=ifelse(x>qnorm(.95),x,NA)),
            params=list(fill="red",alpha=.5, na.rm=TRUE)) )
suppressWarnings(print(g))
```

- Here's the plot. As before, the shaded portion represents the 5% of the area composing the region of rejection. This time, though, it's composed of **two equal pieces**, each containing 2.5% of the area under the curve. The X values in the shaded portions are values which are at least 1.96 standard deviations away from the hypothesized mean.
*Since we want to stick with a 5% rejection rate, we divide it in half and consider values at both tails, at the .025 and the .975 percentiles.  This means that our test statistic  (X'-mu) / (s/sqrt(n)) is  less than .025, Z_(alpha/2), or greater than .975, Z_(1-alpha/2).*  

```{r both.05, echo=FALSE, fig.height=4, fig.width=5}
x <- seq(-8,8, length = 2000)
dat <- data.frame(x=x, y=dnorm(x))
library(ggplot2)
g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5) + xlim(-4, 4) + scale_y_continuous(limits=c(0,max(dat$y)))
suppressWarnings(g <- g+ layer("area", stat="identity", position="identity",mapping = aes(x=ifelse(x>qnorm(.975),x,NA)),
            params=list(fill="red",alpha=.5, na.rm=TRUE)) +
   layer("area", stat="identity", position="identity",mapping = aes(x=ifelse(-9<x & x<qnorm(.025),x,NA)),
                                                   params=list(fill="red",alpha=.5, na.rm=TRUE)) )
suppressWarnings(print(g))
```

### Review

* We have fixed $\alpha$ to be low, so if we reject $H_0$ (either our model is wrong) or there is a low probability that we have made an error
* Since our tests were based on alpha, the probability of a Type I error, we say that  we ``fail to reject H_0`` rather than we "accept H_0". If we fail to reject H_0,  then H_0 could be true OR we just might not have enough data to reject it.
* Statistical significance is not the same as scientific significance
* The region of TS values for which you reject $H_0$ is called the rejection region
  
  
### Z test or T test
* The $Z$ test requires the assumptions of the CLT and for $n$ to be large enough for it to apply
* If $n$ is small, then a Gossett's $T$ test is performed exactly in the same way, with the normal quantiles replaced by the appropriate Student's $T$ quantiles and $n-1$ df
* The probability of rejecting the null hypothesis when it is false is called *power*
* Power is a used a lot to calculate sample sizes for experiments


## Sleep Example - T test
- Consider our example again. Suppose that $n= 16$ (rather than
$100$). Then consider that
$$
.05 = P\left(\frac{\bar X - 30}{s / \sqrt{16}} \geq t_{1-\alpha, 15} ~|~ \mu = 30 \right)
$$
- As before,  (sample mean) `X'=32`,  (standard deviation) `s=10`.  H_0 says the true mean `mu=30`, and H_a is that `mu>30`. With this smaller sample size we use the t test, but our test statistic is computed the same way, namely  `(X'-mu)/(s/sqrt(n))`

- degrees of freedom : `15`,  the quantile `qt(.95,15)` = `r qt(.95,15)`


- So that our test statistic is now $\sqrt{16}(32 - 30) / 10 = 0.8 $, while the critical value is $t_{1-\alpha, 15} = 1.75$. 

- So the test statistic (.8) is less than 1.75, the  95th percentile of the t distribution with 15 df. This means that our sample mean (32) does not fall within the region of rejection since H_a was that mu>30. 


- We now **fail to reject**.


### Two sided tests
* Suppose that we would reject the null hypothesis if in fact the mean was too large or too small
* That is, we want to test the alternative $H_a : \mu \neq 30$ (doesn't make a lot of sense in our setting)
* Then note

$$
 \alpha = P\left(\left. \left|\frac{\bar X - 30}{s /\sqrt{16}}\right| > t_{1-\alpha/2,15} ~\right|~ \mu = 30\right)
$$

* That is we will reject if the test statistic, $0.8$, is either too large or too small, but the critical value is calculated using $\alpha / 2$
* In our example the critical value is `qt(.975,15)` = $2.13$, so we fail to reject.


- Bottom line here is if you **fail** to reject the **one sided test**, you know that you will **fail** to reject the **two sided**.


### T test in R

- Now we usually don't have to do all this computation ourselves because R provides the function t.test which happily does all the work!

- Example:  
We'll do a t test on this paired data to see if fathers and sons have similar heights (our null hypothesis)
```{r fatherTTest, message=FALSE, warning=FALSE, cache=TRUE}
library(UsingR); data(father.son)
t.test(father.son$sheight - father.son$fheight)
```

- The test statistic is: $11.7885$ which is quite large so we **REJECT** the null hypothesis that the true mean of the difference  was 0 (if you ran the test on the difference sheight-fheight) or that the true difference in means was 0 (if you ran the test on the two separate but paired columns).

- The test statistic tell us *how many standard deviations the sample mean is from the hypothesized one*. Remember t=(X'-mu)/(s/sqrt(n))

- We can test this by multiplying the t statistic (11.7885) by the standard deviation of the data divided by the square root of the sample size. 
```{r fathermanualtest}
#n=1078
11.7885 * sd(father.son$sheight -father.son$fheight)/sqrt(1078)
```  

- Note the 95% confidence interval, `0.8310296 1.1629160`, returned by t.test. It does not contain the hypothesized population mean 0 so we're pretty confident we can safely **reject** the hypothesis. This tells us that either our hypothesis is wrong or we're making a mistake (Type 1) in rejecting it.


### hypothesis tests & confidence intervals


- You've probably noticed the strong **similarity** between the **confidence intervals** we studied in the last lesson and these **hypothesis tests**. That's because they're `equivalent`!

- If you set alpha to  some value (say .05) and ran many tests checking alternative hypotheses against H_0, that mu=mu_0, the set of all possible values for which you fail to reject H_0 forms the  (1-alpha)% (that is 95%) confidence interval for mu_0.


- Similarly, if a (1-alpha)% interval contains mu_0, then we fail to reject H_0.


### hypothesis testing and binomial distributions

- Recall this problem, *Suppose a friend has $8$ children, $7$ of which are girls and none are twins*

- Perform the relevant hypothesis test. $H_0 : p = 0.5$ $H_a : p > 0.5$
  - What is the relevant rejection region so that the probability of rejecting is (less than) 5%?
  
- find the probabilities associated with different rejection regions, where a rejection region i has at least i-1 girls out of a possible 8.  
*the expression i-1 here because we want the probability of no. of girls* **more** *than i-1*, that is ` lower.tail = FALSE`

```
#code in the form
Rejection region | Type I error rate |
---|---|
[0 : 8] | `r pbinom(-1, size = 8, p = .5, lower.tail = FALSE)`
[1 : 8] | `r pbinom( 0, size = 8, p = .5, lower.tail = FALSE)`
...

[7 : 8] | `r pbinom( 6, size = 8, p = .5, lower.tail = FALSE)`
[8 : 8] | `r pbinom( 7, size = 8, p = .5, lower.tail = FALSE)`
```


Rejection region | Type I error rate |
---|---|
[0 : 8] | `r pbinom(-1, size = 8, p = .5, lower.tail = FALSE)`
[1 : 8] | `r pbinom( 0, size = 8, p = .5, lower.tail = FALSE)`
[2 : 8] | `r pbinom( 1, size = 8, p = .5, lower.tail = FALSE)`
[3 : 8] | `r pbinom( 2, size = 8, p = .5, lower.tail = FALSE)`
[4 : 8] | `r pbinom( 3, size = 8, p = .5, lower.tail = FALSE)`
[5 : 8] | `r pbinom( 4, size = 8, p = .5, lower.tail = FALSE)`
[6 : 8] | `r pbinom( 5, size = 8, p = .5, lower.tail = FALSE)`
[7 : 8] | `r pbinom( 6, size = 8, p = .5, lower.tail = FALSE)`
[8 : 8] | `r pbinom( 7, size = 8, p = .5, lower.tail = FALSE)`




- meaning that with probability 1 there are at least 0 girls, and .996 is the probability that there's at least 1 girl out of the 8, and so forth. The probabilities decrease as i increases. What is the least value of i for which the probability is less than .05?  
   $8$
  
- So .03 is the probability  of having at least 7 girls out of a sample of size 8 under H_0 (if p actually is .5) which is what our sample has. This is less than .05 so  our sample falls in this region of rejection. So, we **reject H_0**

  
* It's impossible to get an exact 5% level test for this case due to the discreteness of the binomial. 
  * The closest is the rejection region [7 : 8]
  * Any alpha level lower than `r 1 / 2 ^8` is not attainable.
* For larger sample sizes, we could do a normal approximation, but you already knew this.
* Two sided test isn't obvious. 
  * Given a way to do two sided tests, we could take the set of values of $p_0$ for which we fail to reject to get an exact binomial confidence interval (called the Clopper/Pearson interval, BTW)
* For these problems, people always create a P-value (next lecture) rather than computing the rejection region.

---

## P Values


* **Most common** measure of "statistical significance"
* Their ubiquity, along with concern over their interpretation and use makes them controversial among statisticians
 

- However, because they're popular they're used a lot, and often they're **misused** or misinterpreted. In this lecture we'll focus on how to generate them and interpret them correctly.

__Idea__: Suppose nothing is going on - how unusual is it to see the estimate we got?

__Approach__: 

1. Define the hypothetical distribution of a data summary (statistic) when "nothing is going on" (_null hypothesis_)
2. Calculate the summary/statistic with the data we have (_test statistic_)
3. Compare what we calculated to our hypothetical distribution and see if the value is "extreme" (_p-value_)



* The P-value is the probability under the null hypothesis of obtaining evidence as extreme or more extreme than would be observed by chance alone
* If the P-value is **small**, then either $H_0$ is **true** and we have observed a **rare** event or $H_0$ is **false**
*  In our example the $T$ statistic was $0.8$. 
  * What's the probability of getting a $T$ statistic as large as $0.8$?
```{r}
pt(0.8, 15, lower.tail = FALSE) 
```
* Therefore, the probability of seeing evidence as extreme or more extreme than that actually obtained under $H_0$ is `r pt(0.8, 15, lower.tail = FALSE)`



- **Example**: Suppose that you get a t statistic of 2.5 with 15 df testing H_0, (that mu = mu_0) versus an alternative H_a (that mu > mu_0). We want to find the probability of getting a t statistic as large as 2.5.

- We can use the R function **pt**, the distribution function of the t distribution. This function returns one of two probabilities, EITHER the probability of X > q (if lower.tail is FALSE) OR  X <= q (if lower.tail is TRUE), where q is a quantile argument. Here we'll set q=2.5, df=15, lower.tail=FALSE since H_a says that mu>mu_0. We have to gauge the extremity in the direction of H_a. Run this now.

```{r}
pt(2.5, 15, lower.tail=FALSE)
```

- This result tells us that, **if** H_0 were **true**,  we would see this large a test statistic with probability 1% which is rather a small probability.  
So, we **Reject** H_0


### The attained significance level
* Our test statistic in the first example was $2$ for $H_0 : \mu_0  = 30$ versus $H_a:\mu > 30$.
* Notice that we rejected the one sided test when $\alpha = 0.05$, would we reject if $\alpha = 0.01$, how about $0.001$?
* The smallest value for alpha that you still reject the null hypothesis is called the *attained significance level*
* This is equivalent, but philosophically a little different from, the *P-value*


- **Example**: Recall the example from our last lesson in which we computed a `test statistic of 2`. Our H_0 said that` mu_0 = 30` and the alternative H_a that `mu > 30`. Assume we used a Z test (normal distribution). We rejected the one sided test when alpha was set to $0.05$ .

- The quantile associated with this test:
```{r}
qnorm(.95)
```

- We rejected H_0 because our data (the test statistic actually) **favored H_a**. The test statistic 2 (shown by the vertical blue line) falls in the shaded portion of this figure because it exceeds the quantile. As you know, the shaded portion represents 5% of the area under the curve.
 
```{r pValue5, echo=FALSE, fig.height=5, fig.width=4}
x <- seq(-4,4, length = 2000)
dat <- data.frame(x=x, y=dnorm(x))
library(ggplot2)
g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5)+scale_y_continuous(limits=c(0,max(dat$y)))
suppressWarnings(g <- g+ layer("area", stat="identity", position="identity",mapping = aes(x=ifelse(x>qnorm(.95),x,NA)),
            params=list(fill="red",alpha=.5, na.rm=TRUE))) 
suppressWarnings(g <- g + geom_line(aes(x=2.0),size=1.5,colour="blue"))
suppressWarnings(print(g))
```

- Now try the **99th percentile** to see if we would still reject H_0. 
```{r}
qnorm(.99)
```

```{r pValue1, echo=FALSE, fig.height=5, fig.width=4}

x <- seq(-4,4, length = 2000)
dat <- data.frame(x=x, y=dnorm(x))
g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5)+scale_y_continuous(limits=c(0,max(dat$y)))
suppressWarnings(g <- g+ layer("area", stat="identity", position="identity", mapping = aes(x=ifelse(x>qnorm(.99),x,NA)),
            params=list(fill="red",alpha=.5, na.rm=TRUE))) 
suppressWarnings(g <- g + geom_line(aes(x=2.0),size=1.5,colour="blue"))
suppressWarnings(print(g))
```

- Should we reject H_0 if alpha were .01?  
  `No`
  

- So our data (the test statistic) tells us what the **attained significance level** is. This gives us the probability that a random draw from the distribution is less than or equal to the argument. Try with the test statistic value 2. Use the default values for all the other arguments.

```{r}
pnorm(2)
```

- Just as thought, somewhere between .95 (where we rejected) and .99 (where we failed  to reject). That's reassuring.

- Now let's find the **p value** associated with this example. As before, we'll use  pnorm. But this time we'll set the lower.tail argument to FALSE.

```{r}
pnorm(2,lower.tail=FALSE)
```

- This tells us that the **attained level of significance** is about $2$% .



* By reporting a P-value the reader can perform the hypothesis
  test at whatever $\alpha$ level he or she choses
* If the P-value is less than $\alpha$ you reject the null hypothesis 
* For two sided hypothesis test, double the smaller of the two one sided hypothesis test Pvalues. Most software assumes a two-sided test and automatically doubles the p value.


### Revisiting an earlier example

- Suppose a friend has $8$ children, $7$ of which are girls and none are twins
- If each gender has an independent $50$% probability for each birth, what's the probability of getting $7$ or more girls out of $8$ births?

```{r}
choose(8, 7) * .5 ^ 8 + choose(8, 8) * .5 ^ 8 
pbinom(6, size = 8, prob = .5, lower.tail = FALSE)
```


- We see a probability of about .03. Should we reject or fail to reject H_0 if `alpha = .05`?  
   `Reject`

- We see a probability of about .03. Should we reject or fail to reject H_0 if `alpha = .04`?  
   `Reject`

- We see a probability of about .03. Should we reject or fail to reject H_0 if `alpha = .03`?  
   `Fail to reject`


- For the **other side** of the test we want the probability that X<=7, again out of a sample of size 8 with probability .5. Again, we use pbinom, this time with an argument of 7 and lower.tail=TRUE. Try this now.
  
```{r}  
pbinom(7,size=8,prob=.5,lower.tail=TRUE)
```

- So it's pretty likely (probability .996) that out of 8 children you'll have at most 7 girls.  
The p value of this two sided test is 2*the smaller of the two one-sided values.   
In this case the lower value is .035, so 2 *.035 is the p-value for this two-sided test. 

### Poisson example

- Suppose that a hospital has an infection rate of 10 infections per 100 person/days at risk (rate of 0.1) during the last monitoring period.
- Assume that an infection rate of 0.05 is an important benchmark. 
- Given the model, could the observed rate being larger than 0.05 be attributed to chance?
- Under $H_0: \lambda = 0.05$ so that $\lambda_0 100 = 5$
- Consider $H_a: \lambda > 0.05$.

```{r}
#we have to use 9 as the argument since we're looking for a probability of a value greater than the argument
ppois(9, 5, lower.tail = FALSE)
```


- The probability .03 is less than the benchmark of .05, so we **reject null hypothesis**.

