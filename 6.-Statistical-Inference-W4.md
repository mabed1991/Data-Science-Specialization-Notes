---
title: "6. Statistical Inference W4"
author: "Me"
date: "5/14/2020"
output: 
  html_document:
    keep_md: true 
---


## Power
- Power is the **probability of rejecting** the null hypothesis when it is false
- Ergo, power (as it's name would suggest) is a __good__ thing; you want more power
- A type II error (a bad thing, as its name would suggest) is failing to reject the null hypothesis when it's false; the probability of a **type II** error is usually called $\beta$
- The probability of rejecting a false null hypothesis $= 1 - \beta$ . this is POWER!


### Example
- Recall our previous example involving the Respiratory Distress Index and sleep disturbances.
- $H_0: \mu = 30$ versus $H_a: \mu > 30$
- Test statistic under this null hypothesis `(X'-30)/(s/sqrt(n))`

- Then power is 
$$P\left(\frac{\bar X - 30}{s /\sqrt{n}} > t_{1-\alpha,n-1} ~|~ \mu = \mu_a \right)$$
- Note that this is a function that depends on the specific value of $\mu_a$!
- Notice as $\mu_a$ approaches $30$ the power approaches $\alpha$



### Calculating power for Gaussian data

- Power is the probability that the true mean mu is **greater** than the **(1-alpha) quantile** or qnorm(.95). 

- For  simplicity, We're **assuming normality and equal variance**, say sigma^2/n, for both hypotheses, so under H_0, X'~ N(mu_0, sigma^2/n) and under H_a, X'~ N(mu_a, sigma^2/n).


- Assume that $n$ is large and that we know $\sigma$
$$
\begin{align}
1 -\beta & = 
P\left(\frac{\bar X - 30}{\sigma /\sqrt{n}} > z_{1-\alpha} ~|~ \mu = \mu_a \right)\\
& = P\left(\frac{\bar X - \mu_a + \mu_a - 30}{\sigma /\sqrt{n}} > z_{1-\alpha} ~|~ \mu = \mu_a \right)\\ \\
& = P\left(\frac{\bar X - \mu_a}{\sigma /\sqrt{n}} > z_{1-\alpha} - \frac{\mu_a - 30}{\sigma /\sqrt{n}} ~|~ \mu = \mu_a \right)\\ \\
& = P\left(Z > z_{1-\alpha} - \frac{\mu_a - 30}{\sigma /\sqrt{n}} ~|~ \mu = \mu_a \right)\\ \\
\end{align}
$$


#### Gaussian plot
- Consider $H_0 : \mu = \mu_0$ and $H_a : \mu > \mu_0$ with $\mu = \mu_a$ under $H_a$.
- Under $H_0$ the statistic $Z = \frac{\sqrt{n}(\bar X - \mu_0)}{\sigma}$ is $N(0, 1)$
- Under $H_a$ $Z$ is $N\left( \frac{\sqrt{n}(\mu_a - \mu_0)}{\sigma}, 1\right)$
- We reject if $Z > Z_{1-\alpha}$
- Here's a picture with the two distributions. We've drawn a **vertical line** at our favorite spot, at the 95th percentile of the red distribution. To the right of the line lies 5% of the red distribution.

![](6.-Statistical-Inference-W4_files/figure-html/twoDistPlot-1.png)<!-- -->

- The red distribution represents H_0 and the blue distribution represents H_a

- The mean proposed by H_a=  $2$ 

- __POWER__ is how much of the blue distribution lies to the right of that big vertical line   
It's the area under the blue curve (H_a) to the right of the vertical line. 




### Back to Sleep Example

- mua = 32, mu0 = 30 , sigma = 4 , n = $16$ , alpha = .05

![](6.-Statistical-Inference-W4_files/figure-html/twoDist-1.png)<!-- -->

- The red distribution represents H_0 and the blue distribution represents H_a

- The mean proposed by H_a=  $32$

- __POWER__ is how much of the blue distribution lies to the right of that big vertical line   
It's the area under the blue curve (H_a) to the right of the vertical line. 


### POWER in Action
-  Suppose that we wanted to detect a increase in mean RDI
  of at least 2 events / hour (above 30). 
- **Assume normality** and that the sample in question will have a standard deviation of $4$
- What would be the power if we took a sample size of $16$?
  - $Z_{1-\alpha} = 1.645$ 
  - $\frac{\mu_a - 30}{\sigma /\sqrt{n}} = 2 / (4 /\sqrt{16}) = 2$ 
  - $P(Z > 1.645 - 2) = P(Z > -0.355) = 64\%$

```r
pnorm(-0.355, lower.tail = FALSE)
```

```
## [1] 0.6387052
```

```r
z <- qnorm(.95)
pnorm(30+z,mean=32,lower.tail=FALSE)
```

```
## [1] 0.63876
```

- To see power in action, try this manipulate code

```
library(manipulate) 
mu0 = 30 
myplot <- function(sigma, mua, n, alpha){
    g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
    g = g + stat_function(fun=dnorm, geom ="line",args = list(mean = mu0, sd = sigma / sqrt(n)), size = 2, col = "red") 
    g = g + stat_function(fun =dnorm, geom ="line", args = list(mean = mua, sd = sigma/sqrt(n)), size = 2, col = "blue")
    xitc = mu0 +qnorm(1 - alpha)* 2 *sigma/sqrt(n)
    g = g + geom_vline(xintercept=xitc, size = 3)}

manipulate(
    myplot(sigma, mua, n, alpha),
    sigma =slider(1, 10, step = 1, initial = 4),
    mua = slider(30, 35, step = 1, initial = 32),
    n = slider(1, 50, step = 1, initial = 16),
    alpha = slider(0.01, 0.1, step = 0.01, initial= 0.05) 
  )
```





- We've fixed mu_0 at 30, sigma (standard deviation) at 4 and n (sample size) at 16. The **function myplot** just needs an **alternative mean**, mu_a, as argument. 

- Run myplot now with an argument of 34 to see what it does.

```r
myplot(34)
```

![](6.-Statistical-Inference-W4_files/figure-html/unnamed-chunk-2-1.png)<!-- -->

- The distribution represented by H_a **moved to the right**, so almost all (100%) of the blue curve is to the right of the vertical line, indicating that with mu_a=34, the test is more powerful, i.e., there's a higher probability that it's correct to reject the null hypothesis since it appears false. 

- Now try myplot with an argument of 33.3.

```r
  myplot(33.3)
```

![](6.-Statistical-Inference-W4_files/figure-html/unnamed-chunk-3-1.png)<!-- -->


- This isn't as powerful as the test with mu_a=34 but it makes a pretty picture. 

- Now try myplot with an argument of 30.

```r
  myplot(30)
```

![](6.-Statistical-Inference-W4_files/figure-html/unnamed-chunk-4-1.png)<!-- -->

- The power now, the area under the blue curve to the right of the line, is **exactly 5% or alpha**!


- First, power is a function that **depends** on a specific value of an alternative mean, **mu_a**, which is any value *greater than mu_0*, the mean hypothesized by H_0.

- Second, if **mu_a** is much **bigger** than mu_0=30 then the **power** (probability) is **bigger** than if mu_a is close to 30. As mu_a approaches 30, the mean under H_0, the power approaches alpha.

- Just for fun try myplot with an argument of 28.


```r
myplot(28)
```

![](6.-Statistical-Inference-W4_files/figure-html/unnamed-chunk-5-1.png)<!-- -->

- We see that the blue curve has moved to the left of the red, so the area under it, to the right of the line, is less than the 5% under the red curve. This then is even **less powerful** and contradicts H_a so it's not worth looking at.




- Here's a picture of the power curves for different sample sizes. The alternative means, the mu_a's, are plotted along the horizontal axis and power along the vertical.


![](6.-Statistical-Inference-W4_files/figure-html/powerCurveN-1.png)<!-- -->

- It shows that as mu_a gets bigger, it gets more powerful
  
- As sample size gets bigger, it gets more powerful


- Let's look at the **portly distributions**.

![](6.-Statistical-Inference-W4_files/figure-html/twoDistFat-1.png)<!-- -->


- With this standard deviation=2 (fatter distribution) will power be greater or less than with the standard deviation=1?  
    `less than`
  
- To see this

```r
# sd=1
pnorm(30+z,mean=32,sd=1,lower.tail=FALSE)
```

```
## [1] 0.63876
```

```r
#sd=2
pnorm(30+z*2,mean=32,sd=2,lower.tail=FALSE)
```

```
## [1] 0.259511
```

- As variance increases, what happens to power?

![](6.-Statistical-Inference-W4_files/figure-html/powerCurve_sigma-1.png)<!-- -->


- As alpha increases, what happens to power? 

![](6.-Statistical-Inference-W4_files/figure-html/powerCurve_alpha-1.png)<!-- -->



### Equations
- When testing $H_a : \mu > \mu_0$, notice if power is $1 - \beta$, then 
$$1 - \beta = P\left(Z > z_{1-\alpha} - \frac{\mu_a - \mu_0}{\sigma /\sqrt{n}} ~|~ \mu = \mu_a \right) = P(Z > z_{\beta})$$
- This yields the equation
$$z_{1-\alpha} - \frac{\sqrt{n}(\mu_a - \mu_0)}{\sigma} = z_{\beta}$$
- Unknowns: $\mu_a$, $\sigma$, $n$, $\beta$
- Knowns: $\mu_0$, $\alpha$
- Specify any 3 of the unknowns and you can solve for the remainder


### Conclusion
- The calculation for $H_a:\mu < \mu_0$ is similar
- For $H_a: \mu \neq \mu_0$ calculate the one sided power using
  $\alpha / 2$ (this is only approximately right, it excludes the probability of getting a large TS in the opposite direction of the truth)
- Power goes up as $\alpha$ gets larger
- Power of a one sided test is **greater** than the power of the associated two sided test
- Power goes up as $\mu_1$ gets further away from $\mu_0$
- Power goes up as $n$ goes up
- Power doesn't need $\mu_a$, $\sigma$ and $n$, instead only $\frac{\sqrt{n}(\mu_a - \mu_0)}{\sigma}$
  - The quantity $\frac{\mu_a - \mu_0}{\sigma}$ is called the **effect size**, the difference in the means in standard deviation units.
  - Being unit free, it has some hope of interpretability across settings



### T-test power


-  Consider calculating power for a Gossett's $T$ test for our example
-  The power is
  $$
  P\left(\frac{\bar X - \mu_0}{S /\sqrt{n}} > t_{1-\alpha, n-1} ~|~ \mu = \mu_a \right)
  $$
- since the proposed distribution is **not centered at mu_0**, we have to use the **non-central t distribution**. 
- `power.t.test` does this very well
  - Omit one of the arguments and it solves for it



#### Example

- We'll run it three times with the same values for n (16) and alpha (.05) but different delta and standard deviation values.


```r
power.t.test(n = 16, delta = 2 / 4, sd=1, type = "one.sample",  alt = "one.sided")$power
```

```
## [1] 0.6040329
```

```r
power.t.test(n = 16, delta = 2, sd=4, type = "one.sample",  alt = "one.sided")$power
```

```
## [1] 0.6040329
```

```r
power.t.test(n = 16, delta = 100, sd=200, type = "one.sample", alt = "one.sided")$power
```

```
## [1] 0.6040329
```

- So keeping the **effect size** (the ratio delta/sd) constant preserved the power. Let's try a similar experiment except now we'll specify a power we want and solve for the sample size n. 



```r
power.t.test(power = .8, delta = 2 / 4, sd=1, type = "one.sample",  alt = "one.sided")$n
```

```
## [1] 26.13751
```

```r
power.t.test(power = .8, delta = 2, sd=4, type = "one.sample",  alt = "one.sided")$n
```

```
## [1] 26.13751
```

```r
power.t.test(power = .8, delta = 100, sd=200, type = "one.sample", alt = "one.sided")$n
```

```
## [1] 26.13751
```

